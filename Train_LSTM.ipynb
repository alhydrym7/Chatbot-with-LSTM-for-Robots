{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e10b6a",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ee549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e409aa4c",
   "metadata": {},
   "source": [
    "# Read the JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1db88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path ='new_data_Ai.json'\n",
    "file_path = r\"C:\\Users\\asus\\Desktop\\Artificial intelligence\\Third Year\\Scaned S\\Ai-417 Deep\\project\\new_data_Ai.json\"\n",
    "with open(file_path, 'r') as file:\n",
    "    intents = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03072524",
   "metadata": {},
   "source": [
    "# Tokenize & Stem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65263ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "253cdcf0",
   "metadata": {},
   "source": [
    "# Tokenize the patterns and assign labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73121336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# def tokenize(sentence):\n",
    "#     \"\"\"\n",
    "#     split sentence into array of words/tokens\n",
    "#     a token can be a word or punctuation character, or number\n",
    "#     \"\"\"\n",
    "#     return nltk.word_tokenize(sentence)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    if isinstance(sentence, str):\n",
    "        return nltk.word_tokenize(sentence)\n",
    "    else:\n",
    "        return nltk.word_tokenize(str(sentence))\n",
    "\n",
    "def stem(word):\n",
    "    \"\"\"\n",
    "    stemming = find the root form of the word\n",
    "    examples:\n",
    "    words = [\"organize\", \"organizes\", \"organizing\"]\n",
    "    words = [stem(w) for w in words]\n",
    "    -> [\"organ\", \"organ\", \"organ\"]\n",
    "    \"\"\"\n",
    "    return stemmer.stem(word.lower())\n",
    "\n",
    "\n",
    "def bag_of_words(tokenized_sentence, words):\n",
    "    \"\"\"\n",
    "    return bag of words array:\n",
    "    1 for each known word that exists in the sentence, 0 otherwise\n",
    "    example:\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n",
    "    \"\"\"\n",
    "    # stem each word\n",
    "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
    "    # initialize bag with 0 for each word\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in sentence_words: \n",
    "            bag[idx] = 1\n",
    "\n",
    "    return bag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    # add to tag list\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = tokenize(pattern)\n",
    "        # add to our words list\n",
    "        all_words.extend(w)\n",
    "        # add to xy pair\n",
    "        xy.append((w, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b67d4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ignore_words = ['?','.', '!']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "# remove duplicates and sort\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"patterns======> \" , len(xy))\n",
    "print(\"tag=====>\",len(tags))\n",
    "print( \"unique stemmed words=======>\",len(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32cfb18",
   "metadata": {},
   "source": [
    "# Create a Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754763ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tokenized_sentence, words):\n",
    "    \"\"\"\n",
    "    return bag of words array:\n",
    "    1 for each known word that exists in the sentence, 0 otherwise\n",
    "    example:\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
    "\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in sentence_words: \n",
    "            bag[idx] = 1\n",
    "\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0724358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    # X: bag of words for each pattern_sentence\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61097a4f",
   "metadata": {},
   "source": [
    "# Define The LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super(LSTMModel, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         output, _ = self.lstm(input.view(len(input), 1, -1))\n",
    "#         output = self.fc(output.view(len(input), -1))\n",
    "#         return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output, _ = self.gru(input.unsqueeze(0))\n",
    "        output = self.fc(output.squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b115c0",
   "metadata": {},
   "source": [
    "# Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff7dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "print(input_size, output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fbee39",
   "metadata": {},
   "source": [
    "# CustomDataset Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b78d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a744eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset()\n",
    "train_loader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1154f",
   "metadata": {},
   "source": [
    "# Set Device To GPU If Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9b1d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b5d0a4",
   "metadata": {},
   "source": [
    "# Define Loss Function And Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUModel(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f4c26",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_values = []\n",
    "loss_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(torch.float32).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Convert labels to Long data type\n",
    "        labels = labels.long()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * words.size(0)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Compute epoch accuracy and loss\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    \n",
    "    # Append accuracy and loss values to lists\n",
    "    accuracy_values.append(epoch_accuracy)\n",
    "    loss_values.append(epoch_loss)\n",
    "    \n",
    "    # Print epoch accuracy and loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Accuracy: {epoch_accuracy:.2f}%, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "\n",
    "\n",
    "print(f'Final Accuracy: {epoch_accuracy:.2f}')\n",
    "print(f'Final Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6966aaea",
   "metadata": {},
   "source": [
    "# Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31067115",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"input_size\": input_size,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"output_size\": output_size,\n",
    "    \"all_words\": all_words,\n",
    "    \"tags\": tags\n",
    "}\n",
    "\n",
    "FILE = \"Byan_Model_gru.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'Training complete. File saved to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01c0f3",
   "metadata": {},
   "source": [
    "# Plot Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe1ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, num_epochs+1), accuracy_values)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e28e26",
   "metadata": {},
   "source": [
    "# Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2164395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(range(1, num_epochs+1), loss_values)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f2b447",
   "metadata": {},
   "source": [
    "# Test ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e60dfbf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mByan_Model_gru.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mload(FILE)\n\u001b[0;32m      4\u001b[0m input_size \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m hidden_size \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "FILE = \"Byan_Model_gru.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "\n",
    "model = GRUModel(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "conversation = \"\"\n",
    "user_name = \"You: \"\n",
    "bot_name = \"Kharbush: \"\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(f\"{user_name}\")\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    sentence = user_input\n",
    "    sentence = tokenize(sentence)\n",
    "    \n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "\n",
    "            \n",
    "    if prob.item() > 0.75:\n",
    "        for intent in intents['intents']:\n",
    "        #print(intent)\n",
    "                    \n",
    "            if tag == intent[\"tag\"]:\n",
    "                my_resbonce = random.choice(intent['responses'])\n",
    "                #resbonce = MyTranslate_ToAr(resbonce)\n",
    "                print(f\"\\n{bot_name}: {my_resbonce}\")\n",
    "                \n",
    "                \n",
    "                break\n",
    "    else:\n",
    "        print(f\"{bot_name}: I do not understand...\") \n",
    "    print(\"\\n========== \",prob.item(),\" =========\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048976a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea26661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def autocorrect(input_sentence, stored_sentences):\n",
    "    if input_sentence is None:\n",
    "        return \"Sorry, I couldn't understand your input.\"\n",
    "    \n",
    "    similarity_scores = []\n",
    "    for stored_sentence in stored_sentences:\n",
    "        similarity_scores.append(difflib.SequenceMatcher(None, input_sentence, stored_sentence).ratio())\n",
    "    max_score = max(similarity_scores)\n",
    "    if max_score >= 0.5:\n",
    "        index = similarity_scores.index(max_score)\n",
    "        return stored_sentences[index]\n",
    "    else:\n",
    "        return \"Sorry, I couldn't find a match for your sentence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d6e5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path ='new_data_Ai.json'\n",
    "with open(file_path, 'r',encoding=\"utf-8\") as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "patterns_l = [pattern.lower() for intent in intents['intents'] for pattern in intent['patterns']]\n",
    "patterns_c = [pattern for intent in intents['intents'] for pattern in intent['patterns']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18adf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hi\n",
      "ggggggggg =  hi\n",
      "ggggggggg =  hi\n",
      "You: long does casting take?\n",
      "ggggggggg =  mention some clustering methods\n",
      "ggggggggg =  mention some clustering methods\n",
      "You: Do you take credit\n",
      "ggggggggg =  do you like me?\n",
      "ggggggggg =  Do you like me?\n",
      "You: Can I pay with\n",
      "ggggggggg =  Sorry, I couldn't find a match for your sentence.\n",
      "ggggggggg =  Please can I talk with you?\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # sentence = \"do you use credit cards?\"\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    patterns_l2 = autocorrect(user_input,patterns_l)\n",
    "    patterns_c2 = autocorrect(user_input,patterns_c)\n",
    "    \n",
    "    if (patterns_l2 in patterns_l) or (patterns_c2 in patterns_c):\n",
    "        print(\"ggggggggg = \",patterns_l2)\n",
    "        print(\"ggggggggg = \",patterns_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cfb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (patterns_l2 in patterns_l) or (patterns_c2 in patterns_c):\n",
    "        print(\"ggggggggg = \",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e05e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8d117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c9000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873ec5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55994a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb92532a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef143464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc0ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28343345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca907a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a420aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa3e00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c1ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b494e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
